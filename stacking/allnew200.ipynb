{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data shape : (2408, 50)\n",
      "new X_data shape : (2408, 5, 10)\n",
      "y_data shape : (2408, 2)\n",
      "new y_data shape : (2408, 2)\n",
      "[[-1.         -1.        ]\n",
      " [-0.99969315 -0.99884536]\n",
      " [-0.99938229 -0.99769038]\n",
      " ...\n",
      " [ 0.66212312  0.99439013]\n",
      " [ 0.65831461  0.99719455]\n",
      " [ 0.65450609  1.        ]]\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 32)                4224      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 4,290\n",
      "Trainable params: 4,290\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 2s 11ms/step - loss: 0.1834 - val_loss: 0.0390\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0163\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0142 - val_loss: 0.0113\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0105 - val_loss: 0.0090\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0088 - val_loss: 0.0078\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0074 - val_loss: 0.0064\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0064 - val_loss: 0.0055\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0047 - val_loss: 0.0043\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0040 - val_loss: 0.0035\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0026 - val_loss: 0.0023\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0021 - val_loss: 0.0019\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9.6645e-04 - val_loss: 9.9567e-04\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9.2782e-04 - val_loss: 9.5759e-04\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8.6321e-04 - val_loss: 8.6489e-04\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7.9838e-04 - val_loss: 7.5797e-04\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7.6282e-04 - val_loss: 7.5227e-04\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7.1993e-04 - val_loss: 7.0896e-04\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7.0734e-04 - val_loss: 6.8091e-04\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6.4169e-04 - val_loss: 6.2395e-04\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5.8990e-04 - val_loss: 5.9304e-04\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5.7234e-04 - val_loss: 5.7350e-04\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 5.5759e-04 - val_loss: 5.7535e-04\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5.2919e-04 - val_loss: 5.1907e-04\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 4.9620e-04 - val_loss: 5.1035e-04\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.8138e-04 - val_loss: 4.8126e-04\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.5890e-04 - val_loss: 5.1913e-04\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.4196e-04 - val_loss: 4.5182e-04\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.4006e-04 - val_loss: 4.4210e-04\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.1145e-04 - val_loss: 4.3889e-04\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.9181e-04 - val_loss: 4.0491e-04\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.7480e-04 - val_loss: 4.1002e-04\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.7174e-04 - val_loss: 4.3780e-04\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.5032e-04 - val_loss: 3.8546e-04\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.2383e-04 - val_loss: 3.8831e-04\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.3207e-04 - val_loss: 3.5435e-04\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.1000e-04 - val_loss: 3.5563e-04\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.1094e-04 - val_loss: 4.0658e-04\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.1283e-04 - val_loss: 3.2984e-04\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.0436e-04 - val_loss: 3.6745e-04\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.8467e-04 - val_loss: 3.1840e-04\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.7211e-04 - val_loss: 3.3641e-04\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.6129e-04 - val_loss: 3.1246e-04\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.6642e-04 - val_loss: 3.0778e-04\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.5890e-04 - val_loss: 3.0101e-04\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.4351e-04 - val_loss: 3.2357e-04\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.4724e-04 - val_loss: 3.1413e-04\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.3822e-04 - val_loss: 2.8838e-04\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 2.4079e-04 - val_loss: 3.4479e-04\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.2716e-04 - val_loss: 2.9351e-04\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.2257e-04 - val_loss: 2.9172e-04\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.1012e-04 - val_loss: 2.7499e-04\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.3059e-04 - val_loss: 2.8994e-04\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.0798e-04 - val_loss: 2.6821e-04\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.9588e-04 - val_loss: 2.6462e-04\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.8696e-04 - val_loss: 2.4350e-04\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.8308e-04 - val_loss: 2.3685e-04\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.8790e-04 - val_loss: 2.6348e-04\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.8294e-04 - val_loss: 2.3592e-04\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.6834e-04 - val_loss: 2.3873e-04\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.6266e-04 - val_loss: 2.3607e-04\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.5798e-04 - val_loss: 2.0858e-04\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.5492e-04 - val_loss: 2.1365e-04\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.5249e-04 - val_loss: 2.0927e-04\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.5138e-04 - val_loss: 2.0567e-04\n",
      "Epoch 77/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.4078e-04 - val_loss: 2.1262e-04\n",
      "Epoch 78/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.4593e-04 - val_loss: 2.1175e-04\n",
      "Epoch 79/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.3487e-04 - val_loss: 2.1981e-04\n",
      "Epoch 80/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.3809e-04 - val_loss: 2.0032e-04\n",
      "Epoch 81/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.2722e-04 - val_loss: 1.8321e-04\n",
      "Epoch 82/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.3316e-04 - val_loss: 2.0543e-04\n",
      "Epoch 83/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.2952e-04 - val_loss: 1.9118e-04\n",
      "Epoch 84/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.2433e-04 - val_loss: 1.7485e-04\n",
      "Epoch 85/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.0894e-04 - val_loss: 2.0751e-04\n",
      "Epoch 86/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.2305e-04 - val_loss: 1.6432e-04\n",
      "Epoch 87/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.0374e-04 - val_loss: 1.4591e-04\n",
      "Epoch 88/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.0656e-04 - val_loss: 1.6395e-04\n",
      "Epoch 89/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.0531e-04 - val_loss: 1.5150e-04\n",
      "Epoch 90/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9.4734e-05 - val_loss: 1.6831e-04\n",
      "Epoch 91/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9.1370e-05 - val_loss: 1.5014e-04\n",
      "Epoch 92/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 9.3789e-05 - val_loss: 1.6380e-04\n",
      "Epoch 93/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.0080e-04 - val_loss: 1.3809e-04\n",
      "Epoch 94/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 9.1047e-05 - val_loss: 1.3305e-04\n",
      "Epoch 95/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8.5488e-05 - val_loss: 1.4883e-04\n",
      "Epoch 96/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8.2500e-05 - val_loss: 1.1893e-04\n",
      "Epoch 97/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7.5371e-05 - val_loss: 1.7052e-04\n",
      "Epoch 98/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7.9852e-05 - val_loss: 1.2075e-04\n",
      "Epoch 99/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7.6384e-05 - val_loss: 1.1782e-04\n",
      "Epoch 100/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6.8543e-05 - val_loss: 1.1302e-04\n",
      "Epoch 101/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6.6235e-05 - val_loss: 1.2480e-04\n",
      "Epoch 102/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7.5557e-05 - val_loss: 1.3138e-04\n",
      "Epoch 103/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 7.5795e-05 - val_loss: 1.1285e-04\n",
      "Epoch 104/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6.9799e-05 - val_loss: 1.4291e-04\n",
      "Epoch 105/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 6.5155e-05 - val_loss: 1.1872e-04\n",
      "Epoch 106/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5.5239e-05 - val_loss: 1.1198e-04\n",
      "Epoch 107/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5.5306e-05 - val_loss: 9.1066e-05\n",
      "Epoch 108/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6.6621e-05 - val_loss: 1.2097e-04\n",
      "Epoch 109/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 6.4242e-05 - val_loss: 9.9242e-05\n",
      "Epoch 110/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5.3425e-05 - val_loss: 1.0586e-04\n",
      "Epoch 111/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5.7207e-05 - val_loss: 1.0031e-04\n",
      "Epoch 112/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5.3663e-05 - val_loss: 1.0822e-04\n",
      "Epoch 113/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5.4177e-05 - val_loss: 8.6535e-05\n",
      "Epoch 114/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.8769e-05 - val_loss: 8.1761e-05\n",
      "Epoch 115/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5.6391e-05 - val_loss: 8.5860e-05\n",
      "Epoch 116/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5.3937e-05 - val_loss: 7.8462e-05\n",
      "Epoch 117/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.5749e-05 - val_loss: 9.4477e-05\n",
      "Epoch 118/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.5905e-05 - val_loss: 8.7706e-05\n",
      "Epoch 119/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.3637e-05 - val_loss: 9.5674e-05\n",
      "Epoch 120/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.9723e-05 - val_loss: 9.9747e-05\n",
      "Epoch 121/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.4885e-05 - val_loss: 7.6764e-05\n",
      "Epoch 122/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.1062e-05 - val_loss: 7.9990e-05\n",
      "Epoch 123/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.7125e-05 - val_loss: 7.9132e-05\n",
      "Epoch 124/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.3627e-05 - val_loss: 7.1931e-05\n",
      "Epoch 125/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.5162e-05 - val_loss: 7.3441e-05\n",
      "Epoch 126/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.5204e-05 - val_loss: 7.2020e-05\n",
      "Epoch 127/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.3609e-05 - val_loss: 7.6764e-05\n",
      "Epoch 128/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.5105e-05 - val_loss: 6.7161e-05\n",
      "Epoch 129/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.4656e-05 - val_loss: 7.5385e-05\n",
      "Epoch 130/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.3927e-05 - val_loss: 7.5398e-05\n",
      "Epoch 131/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.8011e-05 - val_loss: 6.7577e-05\n",
      "Epoch 132/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.0584e-05 - val_loss: 6.2355e-05\n",
      "Epoch 133/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.1360e-05 - val_loss: 7.3014e-05\n",
      "Epoch 134/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.0269e-05 - val_loss: 9.3753e-05\n",
      "Epoch 135/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.4680e-05 - val_loss: 7.2441e-05\n",
      "Epoch 136/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.7781e-05 - val_loss: 9.3942e-05\n",
      "Epoch 137/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.4254e-05 - val_loss: 7.1987e-05\n",
      "Epoch 138/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.2702e-05 - val_loss: 6.8592e-05\n",
      "Epoch 139/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.3362e-05 - val_loss: 7.4120e-05\n",
      "Epoch 140/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.8558e-05 - val_loss: 5.4140e-05\n",
      "Epoch 141/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.5974e-05 - val_loss: 5.9872e-05\n",
      "Epoch 142/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.9425e-05 - val_loss: 6.6300e-05\n",
      "Epoch 143/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.3749e-05 - val_loss: 7.1073e-05\n",
      "Epoch 144/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.9160e-05 - val_loss: 5.8271e-05\n",
      "Epoch 145/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.6552e-05 - val_loss: 5.9616e-05\n",
      "Epoch 146/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.3053e-05 - val_loss: 6.1651e-05\n",
      "Epoch 147/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.5020e-05 - val_loss: 5.9174e-05\n",
      "Epoch 148/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.8410e-05 - val_loss: 5.5228e-05\n",
      "Epoch 149/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.3214e-05 - val_loss: 5.4826e-05\n",
      "Epoch 150/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.8391e-05 - val_loss: 6.2924e-05\n",
      "Epoch 151/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.3216e-05 - val_loss: 5.3664e-05\n",
      "Epoch 152/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 2.3377e-05 - val_loss: 5.6571e-05\n",
      "Epoch 153/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 2.9320e-05 - val_loss: 5.3580e-05\n",
      "Epoch 154/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 2.7596e-05 - val_loss: 5.3191e-05\n",
      "Epoch 155/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.5719e-05 - val_loss: 5.4804e-05\n",
      "Epoch 156/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.6038e-05 - val_loss: 5.8053e-05\n",
      "Epoch 157/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.5217e-05 - val_loss: 7.7496e-05\n",
      "Epoch 158/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.4468e-05 - val_loss: 5.7190e-05\n",
      "Epoch 159/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.3749e-05 - val_loss: 8.5755e-05\n",
      "Epoch 160/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.3392e-05 - val_loss: 5.7053e-05\n",
      "Epoch 161/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.3091e-05 - val_loss: 5.2953e-05\n",
      "Epoch 162/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.7579e-05 - val_loss: 4.5529e-05\n",
      "Epoch 163/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.2552e-05 - val_loss: 4.4464e-05\n",
      "Epoch 164/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.0617e-05 - val_loss: 4.5624e-05\n",
      "Epoch 165/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.2705e-05 - val_loss: 1.3563e-04\n",
      "Epoch 166/200\n",
      "53/53 [==============================] - 0s 4ms/step - loss: 6.0082e-05 - val_loss: 5.9967e-05\n",
      "Epoch 167/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.0694e-05 - val_loss: 6.6023e-05\n",
      "Epoch 168/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.1265e-05 - val_loss: 4.5111e-05\n",
      "Epoch 169/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.8031e-05 - val_loss: 4.4861e-05\n",
      "Epoch 170/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.0285e-05 - val_loss: 4.7596e-05\n",
      "Epoch 171/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.9273e-05 - val_loss: 4.6915e-05\n",
      "Epoch 172/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.2121e-05 - val_loss: 6.5203e-05\n",
      "Epoch 173/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.3115e-05 - val_loss: 5.7076e-05\n",
      "Epoch 174/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.1307e-05 - val_loss: 4.8367e-05\n",
      "Epoch 175/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.6149e-05 - val_loss: 4.6321e-05\n",
      "Epoch 176/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.3727e-05 - val_loss: 5.3651e-05\n",
      "Epoch 177/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.7269e-05 - val_loss: 4.1098e-05\n",
      "Epoch 178/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.0725e-05 - val_loss: 4.4592e-05\n",
      "Epoch 179/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.9156e-05 - val_loss: 4.3365e-05\n",
      "Epoch 180/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.3796e-05 - val_loss: 6.7829e-05\n",
      "Epoch 181/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.7185e-05 - val_loss: 4.7729e-05\n",
      "Epoch 182/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.7241e-05 - val_loss: 4.1339e-05\n",
      "Epoch 183/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.0661e-05 - val_loss: 4.0665e-05\n",
      "Epoch 184/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.8733e-05 - val_loss: 5.3306e-05\n",
      "Epoch 185/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.0738e-05 - val_loss: 4.3300e-05\n",
      "Epoch 186/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.8749e-05 - val_loss: 4.7108e-05\n",
      "Epoch 187/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.1013e-05 - val_loss: 4.6109e-05\n",
      "Epoch 188/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.6841e-05 - val_loss: 3.9300e-05\n",
      "Epoch 189/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.0597e-05 - val_loss: 3.9587e-05\n",
      "Epoch 190/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.9830e-05 - val_loss: 5.7034e-05\n",
      "Epoch 191/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 3.8081e-05 - val_loss: 1.8406e-04\n",
      "Epoch 192/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.4821e-05 - val_loss: 5.1464e-05\n",
      "Epoch 193/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.1660e-05 - val_loss: 5.8700e-05\n",
      "Epoch 194/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.8682e-05 - val_loss: 3.9927e-05\n",
      "Epoch 195/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.6600e-05 - val_loss: 4.1403e-05\n",
      "Epoch 196/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.0803e-05 - val_loss: 4.2566e-05\n",
      "Epoch 197/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.1442e-05 - val_loss: 4.1438e-05\n",
      "Epoch 198/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.9939e-05 - val_loss: 4.5593e-05\n",
      "Epoch 199/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 2.2343e-05 - val_loss: 3.8414e-05\n",
      "Epoch 200/200\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 1.7982e-05 - val_loss: 4.0996e-05\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers.recurrent import LSTM,RNN\n",
    "from tensorflow.python.keras.layers.core import Dense, Activation, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Flatten, Dense, Embedding, LSTM, Dropout, Activation\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "np.set_printoptions(suppress=True)  # 关闭科学计数法\n",
    "\n",
    "data_csv = pd.read_csv(\"./filtered.csv\")#读取filtered file\n",
    "trajectory = np.array(data_csv, dtype=np.float64)  # trajectory1[:, 2:9] 原为2个数据 现为8个\n",
    "dataX, dataY = [], []\n",
    "    # 定义滑动窗口的大小和步长\n",
    "window_size = 10\n",
    "step_size = 1\n",
    "\n",
    " # 创建输入数据和目标数据\n",
    "\n",
    "    # 创建输入数据和目标数据\n",
    "\n",
    "for i in range(0, len(trajectory) - window_size, step_size):\n",
    "    dataX.append(trajectory[i:i+window_size])\n",
    "    dataY.append(trajectory[i+window_size,1:3])\n",
    "\n",
    " # 将输入数据和目标数据转换为numpy数组\n",
    "dataX = np.array(dataX, dtype='float64')\n",
    "dataY = np.array(dataY, dtype='float64')\n",
    "# dataY = dataY.reshape(2408,1,2)\n",
    "# print('dataX shape:', dataX.shape) # (91, 10, 3)\n",
    "# print('dataY shape:', dataY.shape) # (91, 2)\n",
    "# 使用transpose()方法交换第二维度和第三维度\n",
    "dataX = dataX.transpose((0, 2, 1))\n",
    "# print('dataX shape:', dataX.shape)\n",
    "train_x = dataX\n",
    "train_y= dataY\n",
    "x_data = train_x\n",
    "x_data = np.array(x_data)\n",
    "# x_data = x_data.reshape(19828,5,1)\n",
    "y_data = train_y\n",
    "y_data.shape\n",
    "# Preporcessing Normalizing Valuse\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range=(-1,1))\n",
    "# new_df= x_data.reshape(x_data.shape[0],5)\n",
    "x_data=x_data.reshape(2408,50)\n",
    "new_x_data = sc.fit_transform(x_data)\n",
    "new_x_data = new_x_data.reshape(2408,5,10)\n",
    "print('X_data shape :', x_data.shape)\n",
    "print('new X_data shape :', new_x_data.shape)\n",
    "# y_data=y_data.reshape(2408,2)\n",
    "new_y_data = sc.fit_transform(y_data)\n",
    "# new_y_data = new_y_data.reshape(2408,2)\n",
    "print('y_data shape :', y_data.shape)\n",
    "print('new y_data shape :', new_y_data.shape)\n",
    "# print(new_x_data)\n",
    "print(new_y_data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(new_x_data, new_y_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# 创建GRU模型\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.GRU(units=32, input_shape=(5,10)),\n",
    "    tf.keras.layers.Dense(units=2),\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "# 训练模型\n",
    "\n",
    "history =model.fit(x=x_train, y=y_train, epochs=200,batch_size=32,validation_data=(x_test,y_test))\n",
    "model.save(\"gru.h5\")\n",
    "model = load_model(\"gru.h5\")\n",
    "gru_pred_train = model.predict(x_train)\n",
    "gru_pred_test = model.predict(x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataX shape: (2408, 10, 5)\n",
      "dataY shape: (2408, 2)\n",
      "dataX shape: (2408, 5, 10)\n",
      "[[[     0.          1.          2.     ...      7.          8.\n",
      "        9.    ]\n",
      "  [679864.4007 679864.4159 679864.4315 ... 679864.5083 679864.5238\n",
      "   679864.5391]\n",
      "  [419283.8057 419284.144  419284.4826 ... 419286.1755 419286.5141\n",
      "   419286.8528]\n",
      "  [    11.12       11.12       11.12   ...     11.12       11.12\n",
      "       11.12  ]\n",
      "  [     0.          0.          0.     ...      0.          0.\n",
      "        0.    ]]\n",
      "\n",
      " [[     1.          2.          3.     ...      8.          9.\n",
      "       10.    ]\n",
      "  [679864.4159 679864.4315 679864.4467 ... 679864.5238 679864.5391\n",
      "   679864.5546]\n",
      "  [419284.144  419284.4826 419284.8213 ... 419286.5141 419286.8528\n",
      "   419287.1914]\n",
      "  [    11.12       11.12       11.12   ...     11.12       11.12\n",
      "       11.12  ]\n",
      "  [     0.          0.          0.     ...      0.          0.\n",
      "        0.    ]]\n",
      "\n",
      " [[     2.          3.          4.     ...      9.         10.\n",
      "       11.    ]\n",
      "  [679864.4315 679864.4467 679864.4623 ... 679864.5391 679864.5546\n",
      "   679864.5699]\n",
      "  [419284.4826 419284.8213 419285.1599 ... 419286.8528 419287.1914\n",
      "   419287.53  ]\n",
      "  [    11.12       11.12       11.12   ...     11.12       11.12\n",
      "       11.12  ]\n",
      "  [     0.          0.          0.     ...      0.          0.\n",
      "        0.    ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  2405.       2406.       2407.     ...   2412.       2413.\n",
      "     2414.    ]\n",
      "  [679949.3305 679949.1406 679948.9507 ... 679948.0009 679947.811\n",
      "   679947.6212]\n",
      "  [419863.8233 419864.646  419865.4683 ... 419869.581  419870.4036\n",
      "   419871.226 ]\n",
      "  [    27.74       27.7        27.69   ...     27.69       27.69\n",
      "       27.69  ]\n",
      "  [    -0.38       -0.19        0.     ...      0.          0.\n",
      "        0.    ]]\n",
      "\n",
      " [[  2406.       2407.       2408.     ...   2413.       2414.\n",
      "     2415.    ]\n",
      "  [679949.1406 679948.9507 679948.7608 ... 679947.811  679947.6212\n",
      "   679947.4313]\n",
      "  [419864.646  419865.4683 419866.291  ... 419870.4036 419871.226\n",
      "   419872.0486]\n",
      "  [    27.7        27.69       27.69   ...     27.69       27.69\n",
      "       27.69  ]\n",
      "  [    -0.19        0.          0.     ...      0.          0.\n",
      "        0.    ]]\n",
      "\n",
      " [[  2407.       2408.       2409.     ...   2414.       2415.\n",
      "     2416.    ]\n",
      "  [679948.9507 679948.7608 679948.5709 ... 679947.6212 679947.4313\n",
      "   679947.2414]\n",
      "  [419865.4683 419866.291  419867.1133 ... 419871.226  419872.0486\n",
      "   419872.871 ]\n",
      "  [    27.69       27.69       27.69   ...     27.69       27.69\n",
      "       27.69  ]\n",
      "  [     0.          0.          0.     ...      0.          0.\n",
      "        0.    ]]]\n",
      "[[679864.5546 419287.1914]\n",
      " [679864.5699 419287.53  ]\n",
      " [679864.5854 419287.8687]\n",
      " ...\n",
      " [679947.4313 419872.0486]\n",
      " [679947.2414 419872.871 ]\n",
      " [679947.0515 419873.6937]]\n",
      "X_data shape : (2408, 50)\n",
      "new X_data shape : (2408, 5, 10)\n",
      "y_data shape : (2408, 2)\n",
      "new y_data shape : (2408, 2)\n",
      "[[-1.         -1.        ]\n",
      " [-0.99969315 -0.99884536]\n",
      " [-0.99938229 -0.99769038]\n",
      " ...\n",
      " [ 0.66212312  0.99439013]\n",
      " [ 0.65831461  0.99719455]\n",
      " [ 0.65450609  1.        ]]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 5, 50)             12200     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 5, 50)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5, 100)            60400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 100)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 102,902\n",
      "Trainable params: 102,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "53/53 [==============================] - 7s 22ms/step - loss: 0.2556 - accuracy: 0.6920 - val_loss: 0.0993 - val_accuracy: 0.7621\n",
      "Epoch 2/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.1265 - accuracy: 0.7519 - val_loss: 0.0993 - val_accuracy: 0.9336\n",
      "Epoch 3/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.1231 - accuracy: 0.7377 - val_loss: 0.0918 - val_accuracy: 0.9350\n",
      "Epoch 4/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.1178 - accuracy: 0.7650 - val_loss: 0.0783 - val_accuracy: 0.7538\n",
      "Epoch 5/200\n",
      "53/53 [==============================] - 1s 14ms/step - loss: 0.1131 - accuracy: 0.7715 - val_loss: 0.0804 - val_accuracy: 0.9018\n",
      "Epoch 6/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.1104 - accuracy: 0.7858 - val_loss: 0.0891 - val_accuracy: 0.9668\n",
      "Epoch 7/200\n",
      "53/53 [==============================] - 1s 14ms/step - loss: 0.1045 - accuracy: 0.7935 - val_loss: 0.0728 - val_accuracy: 0.7649\n",
      "Epoch 8/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.1040 - accuracy: 0.8160 - val_loss: 0.0688 - val_accuracy: 0.6957\n",
      "Epoch 9/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0963 - accuracy: 0.8030 - val_loss: 0.0623 - val_accuracy: 0.8741\n",
      "Epoch 10/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0927 - accuracy: 0.8226 - val_loss: 0.0541 - val_accuracy: 0.7842\n",
      "Epoch 11/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0868 - accuracy: 0.8208 - val_loss: 0.0524 - val_accuracy: 0.8880\n",
      "Epoch 12/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0827 - accuracy: 0.8101 - val_loss: 0.0533 - val_accuracy: 0.7842\n",
      "Epoch 13/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0837 - accuracy: 0.8243 - val_loss: 0.0474 - val_accuracy: 0.8326\n",
      "Epoch 14/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0809 - accuracy: 0.8237 - val_loss: 0.0480 - val_accuracy: 0.9419\n",
      "Epoch 15/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0798 - accuracy: 0.8326 - val_loss: 0.0425 - val_accuracy: 0.9723\n",
      "Epoch 16/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0774 - accuracy: 0.8398 - val_loss: 0.0429 - val_accuracy: 0.9350\n",
      "Epoch 17/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0713 - accuracy: 0.8398 - val_loss: 0.0359 - val_accuracy: 0.9156\n",
      "Epoch 18/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0689 - accuracy: 0.8226 - val_loss: 0.0397 - val_accuracy: 0.9474\n",
      "Epoch 19/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0663 - accuracy: 0.8332 - val_loss: 0.0268 - val_accuracy: 0.8852\n",
      "Epoch 20/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0687 - accuracy: 0.8409 - val_loss: 0.0415 - val_accuracy: 0.8797\n",
      "Epoch 21/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0658 - accuracy: 0.8172 - val_loss: 0.0222 - val_accuracy: 0.9322\n",
      "Epoch 22/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0636 - accuracy: 0.8481 - val_loss: 0.0310 - val_accuracy: 0.9862\n",
      "Epoch 23/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0610 - accuracy: 0.8220 - val_loss: 0.0237 - val_accuracy: 0.8728\n",
      "Epoch 24/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0631 - accuracy: 0.8356 - val_loss: 0.0409 - val_accuracy: 0.8631\n",
      "Epoch 25/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0620 - accuracy: 0.8522 - val_loss: 0.0272 - val_accuracy: 0.9959\n",
      "Epoch 26/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0599 - accuracy: 0.8297 - val_loss: 0.0216 - val_accuracy: 0.8797\n",
      "Epoch 27/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0581 - accuracy: 0.8611 - val_loss: 0.0246 - val_accuracy: 0.8700\n",
      "Epoch 28/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0587 - accuracy: 0.8564 - val_loss: 0.0205 - val_accuracy: 0.9820\n",
      "Epoch 29/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0588 - accuracy: 0.8362 - val_loss: 0.0177 - val_accuracy: 0.9931\n",
      "Epoch 30/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0587 - accuracy: 0.8611 - val_loss: 0.0355 - val_accuracy: 0.9931\n",
      "Epoch 31/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0561 - accuracy: 0.8540 - val_loss: 0.0259 - val_accuracy: 0.9945\n",
      "Epoch 32/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0571 - accuracy: 0.8623 - val_loss: 0.0319 - val_accuracy: 0.9308\n",
      "Epoch 33/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0557 - accuracy: 0.8677 - val_loss: 0.0260 - val_accuracy: 0.9959\n",
      "Epoch 34/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0554 - accuracy: 0.8742 - val_loss: 0.0283 - val_accuracy: 0.9931\n",
      "Epoch 35/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0556 - accuracy: 0.8884 - val_loss: 0.0274 - val_accuracy: 0.8064\n",
      "Epoch 36/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0547 - accuracy: 0.8617 - val_loss: 0.0161 - val_accuracy: 0.9876\n",
      "Epoch 37/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0544 - accuracy: 0.8677 - val_loss: 0.0205 - val_accuracy: 0.9945\n",
      "Epoch 38/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0548 - accuracy: 0.8671 - val_loss: 0.0241 - val_accuracy: 0.9945\n",
      "Epoch 39/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0561 - accuracy: 0.8665 - val_loss: 0.0255 - val_accuracy: 0.7967\n",
      "Epoch 40/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0546 - accuracy: 0.8623 - val_loss: 0.0157 - val_accuracy: 0.9959\n",
      "Epoch 41/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0539 - accuracy: 0.8588 - val_loss: 0.0166 - val_accuracy: 0.9876\n",
      "Epoch 42/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0542 - accuracy: 0.8677 - val_loss: 0.0206 - val_accuracy: 0.9959\n",
      "Epoch 43/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0532 - accuracy: 0.8588 - val_loss: 0.0277 - val_accuracy: 0.9945\n",
      "Epoch 44/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0524 - accuracy: 0.8516 - val_loss: 0.0209 - val_accuracy: 0.9917\n",
      "Epoch 45/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0535 - accuracy: 0.8777 - val_loss: 0.0160 - val_accuracy: 0.9986\n",
      "Epoch 46/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0526 - accuracy: 0.8819 - val_loss: 0.0197 - val_accuracy: 0.9848\n",
      "Epoch 47/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0545 - accuracy: 0.8653 - val_loss: 0.0203 - val_accuracy: 0.9986\n",
      "Epoch 48/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0515 - accuracy: 0.8712 - val_loss: 0.0113 - val_accuracy: 0.9945\n",
      "Epoch 49/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0502 - accuracy: 0.8861 - val_loss: 0.0143 - val_accuracy: 0.9959\n",
      "Epoch 50/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0501 - accuracy: 0.8748 - val_loss: 0.0230 - val_accuracy: 0.9959\n",
      "Epoch 51/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0503 - accuracy: 0.8813 - val_loss: 0.0214 - val_accuracy: 0.9945\n",
      "Epoch 52/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0538 - accuracy: 0.8677 - val_loss: 0.0181 - val_accuracy: 0.9931\n",
      "Epoch 53/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0505 - accuracy: 0.8849 - val_loss: 0.0300 - val_accuracy: 0.9959\n",
      "Epoch 54/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0534 - accuracy: 0.8623 - val_loss: 0.0193 - val_accuracy: 0.9972\n",
      "Epoch 55/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0508 - accuracy: 0.8694 - val_loss: 0.0279 - val_accuracy: 0.9972\n",
      "Epoch 56/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0506 - accuracy: 0.8801 - val_loss: 0.0172 - val_accuracy: 0.9876\n",
      "Epoch 57/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0518 - accuracy: 0.8766 - val_loss: 0.0188 - val_accuracy: 0.9931\n",
      "Epoch 58/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0497 - accuracy: 0.8825 - val_loss: 0.0137 - val_accuracy: 0.9931\n",
      "Epoch 59/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0511 - accuracy: 0.8754 - val_loss: 0.0235 - val_accuracy: 0.9945\n",
      "Epoch 60/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0521 - accuracy: 0.8617 - val_loss: 0.0183 - val_accuracy: 0.9931\n",
      "Epoch 61/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0494 - accuracy: 0.8588 - val_loss: 0.0182 - val_accuracy: 0.9959\n",
      "Epoch 62/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0492 - accuracy: 0.8825 - val_loss: 0.0155 - val_accuracy: 0.9972\n",
      "Epoch 63/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0505 - accuracy: 0.8819 - val_loss: 0.0230 - val_accuracy: 0.9945\n",
      "Epoch 64/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0505 - accuracy: 0.8588 - val_loss: 0.0178 - val_accuracy: 0.7981\n",
      "Epoch 65/200\n",
      "53/53 [==============================] - 1s 14ms/step - loss: 0.0485 - accuracy: 0.8564 - val_loss: 0.0120 - val_accuracy: 0.9945\n",
      "Epoch 66/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0490 - accuracy: 0.8985 - val_loss: 0.0212 - val_accuracy: 0.9931\n",
      "Epoch 67/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0501 - accuracy: 0.8718 - val_loss: 0.0148 - val_accuracy: 0.9917\n",
      "Epoch 68/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0491 - accuracy: 0.8926 - val_loss: 0.0123 - val_accuracy: 0.9889\n",
      "Epoch 69/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0483 - accuracy: 0.8677 - val_loss: 0.0160 - val_accuracy: 0.9959\n",
      "Epoch 70/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0501 - accuracy: 0.8777 - val_loss: 0.0243 - val_accuracy: 0.9959\n",
      "Epoch 71/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0497 - accuracy: 0.8950 - val_loss: 0.0248 - val_accuracy: 0.9046\n",
      "Epoch 72/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0513 - accuracy: 0.8694 - val_loss: 0.0219 - val_accuracy: 0.9502\n",
      "Epoch 73/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0486 - accuracy: 0.8665 - val_loss: 0.0201 - val_accuracy: 0.9945\n",
      "Epoch 74/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0502 - accuracy: 0.8777 - val_loss: 0.0232 - val_accuracy: 0.9917\n",
      "Epoch 75/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0501 - accuracy: 0.8760 - val_loss: 0.0190 - val_accuracy: 0.9917\n",
      "Epoch 76/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0477 - accuracy: 0.8694 - val_loss: 0.0167 - val_accuracy: 0.9903\n",
      "Epoch 77/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0486 - accuracy: 0.8510 - val_loss: 0.0216 - val_accuracy: 0.9945\n",
      "Epoch 78/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0489 - accuracy: 0.8825 - val_loss: 0.0155 - val_accuracy: 0.9917\n",
      "Epoch 79/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0495 - accuracy: 0.8564 - val_loss: 0.0119 - val_accuracy: 0.9931\n",
      "Epoch 80/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0480 - accuracy: 0.8801 - val_loss: 0.0139 - val_accuracy: 0.9986\n",
      "Epoch 81/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0478 - accuracy: 0.8499 - val_loss: 0.0191 - val_accuracy: 0.9903\n",
      "Epoch 82/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0475 - accuracy: 0.8641 - val_loss: 0.0131 - val_accuracy: 0.9959\n",
      "Epoch 83/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0485 - accuracy: 0.8593 - val_loss: 0.0177 - val_accuracy: 0.9903\n",
      "Epoch 84/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0470 - accuracy: 0.8801 - val_loss: 0.0321 - val_accuracy: 0.9903\n",
      "Epoch 85/200\n",
      "53/53 [==============================] - 1s 14ms/step - loss: 0.0489 - accuracy: 0.8688 - val_loss: 0.0219 - val_accuracy: 0.9834\n",
      "Epoch 86/200\n",
      "53/53 [==============================] - 1s 14ms/step - loss: 0.0465 - accuracy: 0.8795 - val_loss: 0.0128 - val_accuracy: 0.9917\n",
      "Epoch 87/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0499 - accuracy: 0.8742 - val_loss: 0.0157 - val_accuracy: 0.9931\n",
      "Epoch 88/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0479 - accuracy: 0.8677 - val_loss: 0.0129 - val_accuracy: 0.9931\n",
      "Epoch 89/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0462 - accuracy: 0.8766 - val_loss: 0.0167 - val_accuracy: 0.9959\n",
      "Epoch 90/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0469 - accuracy: 0.8766 - val_loss: 0.0146 - val_accuracy: 0.9959\n",
      "Epoch 91/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0471 - accuracy: 0.8588 - val_loss: 0.0100 - val_accuracy: 0.9945\n",
      "Epoch 92/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0452 - accuracy: 0.8724 - val_loss: 0.0112 - val_accuracy: 0.9931\n",
      "Epoch 93/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0468 - accuracy: 0.8629 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0476 - accuracy: 0.8783 - val_loss: 0.0147 - val_accuracy: 0.9461\n",
      "Epoch 95/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0479 - accuracy: 0.8760 - val_loss: 0.0161 - val_accuracy: 0.7967\n",
      "Epoch 96/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0458 - accuracy: 0.8576 - val_loss: 0.0120 - val_accuracy: 0.9972\n",
      "Epoch 97/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0463 - accuracy: 0.8487 - val_loss: 0.0237 - val_accuracy: 0.9765\n",
      "Epoch 98/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0467 - accuracy: 0.8576 - val_loss: 0.0136 - val_accuracy: 0.9917\n",
      "Epoch 99/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0449 - accuracy: 0.8742 - val_loss: 0.0180 - val_accuracy: 0.9917\n",
      "Epoch 100/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0476 - accuracy: 0.8611 - val_loss: 0.0155 - val_accuracy: 0.9917\n",
      "Epoch 101/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0465 - accuracy: 0.8540 - val_loss: 0.0178 - val_accuracy: 0.9959\n",
      "Epoch 102/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0481 - accuracy: 0.8795 - val_loss: 0.0126 - val_accuracy: 0.9931\n",
      "Epoch 103/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0462 - accuracy: 0.8617 - val_loss: 0.0156 - val_accuracy: 0.9917\n",
      "Epoch 104/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0461 - accuracy: 0.8688 - val_loss: 0.0129 - val_accuracy: 0.9862\n",
      "Epoch 105/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0471 - accuracy: 0.8629 - val_loss: 0.0202 - val_accuracy: 0.9945\n",
      "Epoch 106/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0457 - accuracy: 0.8748 - val_loss: 0.0172 - val_accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0460 - accuracy: 0.8884 - val_loss: 0.0114 - val_accuracy: 0.9903\n",
      "Epoch 108/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0465 - accuracy: 0.8682 - val_loss: 0.0341 - val_accuracy: 0.9917\n",
      "Epoch 109/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0462 - accuracy: 0.8950 - val_loss: 0.0149 - val_accuracy: 0.9986\n",
      "Epoch 110/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0451 - accuracy: 0.8647 - val_loss: 0.0127 - val_accuracy: 0.9931\n",
      "Epoch 111/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0441 - accuracy: 0.8866 - val_loss: 0.0156 - val_accuracy: 0.9931\n",
      "Epoch 112/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0459 - accuracy: 0.8694 - val_loss: 0.0092 - val_accuracy: 0.9917\n",
      "Epoch 113/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0456 - accuracy: 0.8629 - val_loss: 0.0118 - val_accuracy: 0.9945\n",
      "Epoch 114/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0452 - accuracy: 0.8795 - val_loss: 0.0125 - val_accuracy: 0.7981\n",
      "Epoch 115/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0442 - accuracy: 0.8855 - val_loss: 0.0197 - val_accuracy: 0.9945\n",
      "Epoch 116/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0461 - accuracy: 0.8801 - val_loss: 0.0092 - val_accuracy: 0.9931\n",
      "Epoch 117/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0463 - accuracy: 0.8588 - val_loss: 0.0138 - val_accuracy: 0.9931\n",
      "Epoch 118/200\n",
      "53/53 [==============================] - 1s 14ms/step - loss: 0.0454 - accuracy: 0.8421 - val_loss: 0.0122 - val_accuracy: 0.7953\n",
      "Epoch 119/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0449 - accuracy: 0.8795 - val_loss: 0.0186 - val_accuracy: 0.9945\n",
      "Epoch 120/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0447 - accuracy: 0.8795 - val_loss: 0.0150 - val_accuracy: 0.9806\n",
      "Epoch 121/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0446 - accuracy: 0.8688 - val_loss: 0.0130 - val_accuracy: 0.9931\n",
      "Epoch 122/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0462 - accuracy: 0.8653 - val_loss: 0.0154 - val_accuracy: 0.9876\n",
      "Epoch 123/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0452 - accuracy: 0.8819 - val_loss: 0.0206 - val_accuracy: 0.9903\n",
      "Epoch 124/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0440 - accuracy: 0.8469 - val_loss: 0.0092 - val_accuracy: 0.9931\n",
      "Epoch 125/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0446 - accuracy: 0.8807 - val_loss: 0.0147 - val_accuracy: 0.9917\n",
      "Epoch 126/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0452 - accuracy: 0.8605 - val_loss: 0.0139 - val_accuracy: 0.9876\n",
      "Epoch 127/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0451 - accuracy: 0.8629 - val_loss: 0.0151 - val_accuracy: 0.9986\n",
      "Epoch 128/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0435 - accuracy: 0.8641 - val_loss: 0.0119 - val_accuracy: 0.9876\n",
      "Epoch 129/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0436 - accuracy: 0.8783 - val_loss: 0.0130 - val_accuracy: 0.9972\n",
      "Epoch 130/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0448 - accuracy: 0.8582 - val_loss: 0.0175 - val_accuracy: 0.7939\n",
      "Epoch 131/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0466 - accuracy: 0.8718 - val_loss: 0.0138 - val_accuracy: 0.9931\n",
      "Epoch 132/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0448 - accuracy: 0.8736 - val_loss: 0.0133 - val_accuracy: 0.9917\n",
      "Epoch 133/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0436 - accuracy: 0.8623 - val_loss: 0.0114 - val_accuracy: 0.9986\n",
      "Epoch 134/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0435 - accuracy: 0.8748 - val_loss: 0.0168 - val_accuracy: 0.9931\n",
      "Epoch 135/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0462 - accuracy: 0.8659 - val_loss: 0.0122 - val_accuracy: 0.9917\n",
      "Epoch 136/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0450 - accuracy: 0.8700 - val_loss: 0.0185 - val_accuracy: 0.9917\n",
      "Epoch 137/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0432 - accuracy: 0.8688 - val_loss: 0.0154 - val_accuracy: 0.9917\n",
      "Epoch 138/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0451 - accuracy: 0.8760 - val_loss: 0.0136 - val_accuracy: 0.9972\n",
      "Epoch 139/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0433 - accuracy: 0.8409 - val_loss: 0.0155 - val_accuracy: 0.9917\n",
      "Epoch 140/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0437 - accuracy: 0.8599 - val_loss: 0.0182 - val_accuracy: 0.9917\n",
      "Epoch 141/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0460 - accuracy: 0.8736 - val_loss: 0.0100 - val_accuracy: 0.9889\n",
      "Epoch 142/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0455 - accuracy: 0.8623 - val_loss: 0.0205 - val_accuracy: 0.9931\n",
      "Epoch 143/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0435 - accuracy: 0.8528 - val_loss: 0.0141 - val_accuracy: 0.9806\n",
      "Epoch 144/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0440 - accuracy: 0.8635 - val_loss: 0.0170 - val_accuracy: 0.9959\n",
      "Epoch 145/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0460 - accuracy: 0.8665 - val_loss: 0.0137 - val_accuracy: 0.9903\n",
      "Epoch 146/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0447 - accuracy: 0.8825 - val_loss: 0.0168 - val_accuracy: 0.9959\n",
      "Epoch 147/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0442 - accuracy: 0.8528 - val_loss: 0.0104 - val_accuracy: 0.9184\n",
      "Epoch 148/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0448 - accuracy: 0.8546 - val_loss: 0.0099 - val_accuracy: 0.9945\n",
      "Epoch 149/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0425 - accuracy: 0.8849 - val_loss: 0.0097 - val_accuracy: 0.9972\n",
      "Epoch 150/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0430 - accuracy: 0.8588 - val_loss: 0.0172 - val_accuracy: 0.9917\n",
      "Epoch 151/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0443 - accuracy: 0.8682 - val_loss: 0.0120 - val_accuracy: 0.9959\n",
      "Epoch 152/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0452 - accuracy: 0.8623 - val_loss: 0.0205 - val_accuracy: 0.9986\n",
      "Epoch 153/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0451 - accuracy: 0.8588 - val_loss: 0.0174 - val_accuracy: 0.9945\n",
      "Epoch 154/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0441 - accuracy: 0.8540 - val_loss: 0.0125 - val_accuracy: 0.9917\n",
      "Epoch 155/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0416 - accuracy: 0.8712 - val_loss: 0.0157 - val_accuracy: 0.9931\n",
      "Epoch 156/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0437 - accuracy: 0.8754 - val_loss: 0.0143 - val_accuracy: 0.9945\n",
      "Epoch 157/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0433 - accuracy: 0.8718 - val_loss: 0.0110 - val_accuracy: 0.9903\n",
      "Epoch 158/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0424 - accuracy: 0.8724 - val_loss: 0.0091 - val_accuracy: 0.9945\n",
      "Epoch 159/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0436 - accuracy: 0.8724 - val_loss: 0.0243 - val_accuracy: 0.9917\n",
      "Epoch 160/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0443 - accuracy: 0.8564 - val_loss: 0.0112 - val_accuracy: 0.9917\n",
      "Epoch 161/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0458 - accuracy: 0.8457 - val_loss: 0.0210 - val_accuracy: 0.9945\n",
      "Epoch 162/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0442 - accuracy: 0.8647 - val_loss: 0.0111 - val_accuracy: 0.9986\n",
      "Epoch 163/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0425 - accuracy: 0.8635 - val_loss: 0.0136 - val_accuracy: 0.9903\n",
      "Epoch 164/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0432 - accuracy: 0.8528 - val_loss: 0.0181 - val_accuracy: 0.9945\n",
      "Epoch 165/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0440 - accuracy: 0.8588 - val_loss: 0.0158 - val_accuracy: 0.9945\n",
      "Epoch 166/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0427 - accuracy: 0.8611 - val_loss: 0.0091 - val_accuracy: 0.9959\n",
      "Epoch 167/200\n",
      "53/53 [==============================] - 1s 15ms/step - loss: 0.0439 - accuracy: 0.8582 - val_loss: 0.0117 - val_accuracy: 0.9917\n",
      "Epoch 168/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0442 - accuracy: 0.8546 - val_loss: 0.0123 - val_accuracy: 0.9959\n",
      "Epoch 169/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0418 - accuracy: 0.8635 - val_loss: 0.0114 - val_accuracy: 0.9945\n",
      "Epoch 170/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0439 - accuracy: 0.8564 - val_loss: 0.0131 - val_accuracy: 0.9972\n",
      "Epoch 171/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0429 - accuracy: 0.8599 - val_loss: 0.0143 - val_accuracy: 0.9862\n",
      "Epoch 172/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0435 - accuracy: 0.8504 - val_loss: 0.0116 - val_accuracy: 0.9931\n",
      "Epoch 173/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0425 - accuracy: 0.8499 - val_loss: 0.0122 - val_accuracy: 0.9972\n",
      "Epoch 174/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0439 - accuracy: 0.8724 - val_loss: 0.0186 - val_accuracy: 0.9986\n",
      "Epoch 175/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0426 - accuracy: 0.8718 - val_loss: 0.0168 - val_accuracy: 0.9889\n",
      "Epoch 176/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0427 - accuracy: 0.8534 - val_loss: 0.0112 - val_accuracy: 0.9917\n",
      "Epoch 177/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0434 - accuracy: 0.8392 - val_loss: 0.0108 - val_accuracy: 0.9945\n",
      "Epoch 178/200\n",
      "53/53 [==============================] - 1s 14ms/step - loss: 0.0432 - accuracy: 0.8564 - val_loss: 0.0232 - val_accuracy: 0.9959\n",
      "Epoch 179/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0456 - accuracy: 0.8617 - val_loss: 0.0181 - val_accuracy: 0.9959\n",
      "Epoch 180/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0427 - accuracy: 0.8576 - val_loss: 0.0140 - val_accuracy: 0.9322\n",
      "Epoch 181/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0427 - accuracy: 0.8617 - val_loss: 0.0161 - val_accuracy: 0.9959\n",
      "Epoch 182/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0427 - accuracy: 0.8659 - val_loss: 0.0176 - val_accuracy: 0.9945\n",
      "Epoch 183/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0442 - accuracy: 0.8522 - val_loss: 0.0123 - val_accuracy: 0.7953\n",
      "Epoch 184/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0430 - accuracy: 0.8772 - val_loss: 0.0092 - val_accuracy: 0.9917\n",
      "Epoch 185/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0433 - accuracy: 0.8599 - val_loss: 0.0232 - val_accuracy: 0.9917\n",
      "Epoch 186/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0435 - accuracy: 0.8451 - val_loss: 0.0090 - val_accuracy: 0.9959\n",
      "Epoch 187/200\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0415 - accuracy: 0.8593 - val_loss: 0.0104 - val_accuracy: 0.9959\n",
      "Epoch 188/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0426 - accuracy: 0.8522 - val_loss: 0.0149 - val_accuracy: 0.7953\n",
      "Epoch 189/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0420 - accuracy: 0.8588 - val_loss: 0.0236 - val_accuracy: 0.9931\n",
      "Epoch 190/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0435 - accuracy: 0.8534 - val_loss: 0.0080 - val_accuracy: 0.9959\n",
      "Epoch 191/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0415 - accuracy: 0.8528 - val_loss: 0.0208 - val_accuracy: 0.9876\n",
      "Epoch 192/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0429 - accuracy: 0.8463 - val_loss: 0.0152 - val_accuracy: 0.9917\n",
      "Epoch 193/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0427 - accuracy: 0.8617 - val_loss: 0.0077 - val_accuracy: 0.9931\n",
      "Epoch 194/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0434 - accuracy: 0.8528 - val_loss: 0.0109 - val_accuracy: 0.9350\n",
      "Epoch 195/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0420 - accuracy: 0.8481 - val_loss: 0.0158 - val_accuracy: 0.9889\n",
      "Epoch 196/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0427 - accuracy: 0.8706 - val_loss: 0.0104 - val_accuracy: 0.9931\n",
      "Epoch 197/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0411 - accuracy: 0.8659 - val_loss: 0.0108 - val_accuracy: 0.9848\n",
      "Epoch 198/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0420 - accuracy: 0.8516 - val_loss: 0.0208 - val_accuracy: 0.9862\n",
      "Epoch 199/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0417 - accuracy: 0.8588 - val_loss: 0.0117 - val_accuracy: 0.9959\n",
      "Epoch 200/200\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0440 - accuracy: 0.8528 - val_loss: 0.0140 - val_accuracy: 0.9986\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers.recurrent import LSTM,RNN\n",
    "from tensorflow.python.keras.layers.core import Dense, Activation, Dropout\n",
    "from keras import regularizers\n",
    "\n",
    "# from keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.set_printoptions(suppress=True)  # 关闭科学计数法\n",
    "\n",
    "data_csv = pd.read_csv(\"./filtered.csv\")#读取filtered file\n",
    "trajectory = np.array(data_csv, dtype=np.float64)  # trajectory1[:, 2:9] 原为2个数据 现为8个\n",
    "dataX, dataY = [], []\n",
    "    # 定义滑动窗口的大小和步长\n",
    "window_size = 10\n",
    "step_size = 1\n",
    "# 创建输入数据和目标数据\n",
    "# 创建输入数据和目标数据\n",
    "for i in range(0, len(trajectory) - window_size, step_size):\n",
    "    dataX.append(trajectory[i:i+window_size])\n",
    "    dataY.append(trajectory[i+window_size,1:3])\n",
    "\n",
    "# 将输入数据和目标数据转换为numpy数组\n",
    "dataX = np.array(dataX, dtype='float64')\n",
    "dataY = np.array(dataY, dtype='float64')\n",
    "# dataY = dataY.reshape(2408,1,2)\n",
    "print('dataX shape:', dataX.shape) # (91, 10, 3)\n",
    "print('dataY shape:', dataY.shape) # (91, 2)\n",
    "# 使用transpose()方法交换第二维度和第三维度\n",
    "dataX = dataX.transpose((0, 2, 1))\n",
    "print('dataX shape:', dataX.shape)\n",
    "train_x = dataX\n",
    "train_y= dataY\n",
    "print(train_x)\n",
    "print(train_y)\n",
    "x_data = train_x\n",
    "x_data = np.array(x_data)\n",
    "# x_data = x_data.reshape(19828,5,1)\n",
    "x_data.shape\n",
    "y_data = train_y\n",
    "\n",
    "y_data.shape\n",
    "# Preporcessing Normalizing Valuse\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range=(-1,1))\n",
    "# new_df= x_data.reshape(x_data.shape[0],5)\n",
    "x_data=x_data.reshape(2408,50)\n",
    "new_x_data = sc.fit_transform(x_data)\n",
    "new_x_data = new_x_data.reshape(2408,5,10)\n",
    "\n",
    "print('X_data shape :', x_data.shape)\n",
    "print('new X_data shape :', new_x_data.shape)\n",
    "\n",
    "\n",
    "# y_data=y_data.reshape(2408,2)\n",
    "new_y_data = sc.fit_transform(y_data)\n",
    "# new_y_data = new_y_data.reshape(2408,2)\n",
    "print('y_data shape :', y_data.shape)\n",
    "print('new y_data shape :', new_y_data.shape)\n",
    "# print(new_x_data)\n",
    "print(new_y_data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(new_x_data, new_y_data, test_size=0.3, random_state=42)\n",
    "model = Sequential()\n",
    "\n",
    "# first layer\n",
    "# model.add(LSTM(units=50, batch_input_shape=(None,5,10),return_sequences=True,kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(LSTM(units=50, batch_input_shape=(None,5,10),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# second layer\n",
    "# model.add(Dense(128, 1))\n",
    "# model.add(LSTM(units=100,return_sequences=True,kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(LSTM(units=100,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# third layer\n",
    "# model.add(LSTM(units=50,return_sequences=False,kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(LSTM(units=50,return_sequences=False))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# fourth dense layer\n",
    "model.add(Dense(units=2))\n",
    "#不声明默认为0.001\n",
    "# adam = Adam(learning_rate=0.0015)#减小学习率：可以通过将优化器的learning_rate参数设置为一个较小的值来减小学习率。例如，可以将模型编译代码修改为：\n",
    "\n",
    "model.compile(loss='mean_absolute_error',optimizer='adam',metrics=['accuracy'])#增加L2正则化项：将kernel_regularizer参数设置为regularizers.l2()，并指定相应的正则化系数。\n",
    "model.summary()\n",
    "\n",
    "# # 定义LSTM模型\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(64, input_shape=(10, 5)))\n",
    "# model.add(Dense(2))\n",
    "\n",
    "# # 编译模型\n",
    "# model.compile(loss='mse', optimizer='adam')\n",
    "# model.summary()\n",
    "'''\n",
    "# LSTM层的units数：增加units数可以增加模型的表达能力，但也会增加模型的复杂度和训练时间。因此，可以尝试不同的units数并找到一个合适的值。\n",
    "\n",
    "# Dropout层的比率：增加Dropout比率可以减少过拟合的风险，但过高的Dropout比率会影响模型的性能。因此，可以尝试不同的Dropout比率并找到一个合适的值。\n",
    "\n",
    "# 学习率(learning rate)：Adam优化器默认的学习率通常可以正常工作，但有时候需要手动调整学习率以加速或稳定训练过程。\n",
    "\n",
    "# 批量大小(batch size)：批量大小会影响模型的训练速度和内存占用情况。通常情况下，使用大批量大小可以加快训练速度，但也会占用更多的内存。\n",
    "\n",
    "# 训练轮数(epochs)：增加训练轮数可以提高模型的精度，但也会增加训练时间。可以使用早停法(early stopping)等技术来提高模型的训练效率。\n",
    " '''\n",
    "history = model.fit(x_train,y_train,epochs=200, batch_size=32,validation_data=(x_test,y_test))\n",
    "model.save(\"LSTM.h5\")\n",
    "model1 = load_model(\"LSTM.h5\")\n",
    "lstm_pred_train = model.predict(x_train)\n",
    "lstm_pred_test = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.83084186  0.85853785]\n",
      " [ 0.83084186  0.85853785]\n",
      " [-0.71310374 -0.41425886]\n",
      " ...\n",
      " [ 0.83086227  0.85848147]\n",
      " [ 0.83085012  0.85851547]\n",
      " [ 0.83084186  0.85853785]]\n"
     ]
    }
   ],
   "source": [
    "# 导入需要的库\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "# 原数据\n",
    "# x_train, x_test, y_train, y_test\n",
    "# 加载数据集和GRU、LSTM模型的预测结果\n",
    "\n",
    "\n",
    "# gru_pred_train = load_gru_pred_train()  # GRU模型的训练集预测结果 输出\n",
    "# gru_pred_test = load_gru_pred_test()    # GRU模型的测试集预测结果 输出\n",
    "\n",
    "# lstm_pred_train = load_lstm_pred_train()  # LSTM模型的训练集预测结果 输出\n",
    "# lstm_pred_test = load_lstm_pred_test()    # LSTM模型的测试集预测结果 输出\n",
    "x_train=x_train.reshape(-1,50)\n",
    "x_test=x_test.reshape(-1,50)\n",
    "\n",
    "# 将GRU和LSTM的预测结果作为特征融合在一起\n",
    "X_train = np.column_stack((x_train, gru_pred_train, lstm_pred_train))\n",
    "X_test = np.column_stack((x_test, gru_pred_test, lstm_pred_test))\n",
    "\n",
    "# 使用随机森林模型拟合数据\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "y_pred = rf.predict(X_test)\n",
    "print(y_pred)\n",
    "# 评估模型\n",
    "# evaluation(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(723, 2)\n",
      "[[679955.84394513 419832.20976238]\n",
      " [679955.84394513 419832.20976238]\n",
      " [679878.85980691 419458.96066182]\n",
      " ...\n",
      " [679955.84496274 419832.19322956]\n",
      " [679955.84435727 419832.20319948]\n",
      " [679955.84394513 419832.20976238]]\n"
     ]
    }
   ],
   "source": [
    "result= y_pred\n",
    "# # 设置打印选项，精度为3位小数\n",
    "# np.set_printoptions(precision=5, suppress=True)\n",
    "\n",
    "res_df = sc.inverse_transform(result)\n",
    "# res_df.reshape(new_df.shape[0],5,1)\n",
    "res_df\n",
    "print(res_df.shape)\n",
    "print(res_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[679955.8413 419832.253 ]\n",
      " [679955.8419 419832.2448]\n",
      " [679878.8793 419458.8907]\n",
      " ...\n",
      " [679955.8474 419832.1637]\n",
      " [679955.844  419832.2116]\n",
      " [679955.8437 419832.2183]]\n",
      "(723, 2)\n"
     ]
    }
   ],
   "source": [
    "y_test=y_test.reshape (723, 2)  #需要改的地方\n",
    "# y_test_actual=sc.inverse_transform(y_test[2:3])\n",
    "y_test_actual=sc.inverse_transform(y_test)\n",
    "print(y_test_actual)\n",
    "print(y_test_actual.shape)\n",
    "# for i in range(len(result)):\n",
    "#     plt.scatter(result[0][i],result[1][i],c='r')\n",
    "#     plt.scatter(y_test[0][i],y_test[1][i],c='g')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制预测值和实际值\n",
    "\n",
    "predicted_df = pd.DataFrame(data=res_df[0:,0:],index=[i for i in range(res_df.shape[0])],columns=['f'+str(i) for i in range(res_df.shape[1])])\n",
    "\n",
    "actual_df = pd.DataFrame(data=y_test_actual[0:,0:],index=[i for i in range(y_test_actual.shape[0])],columns=['f'+str(i) for i in range(y_test_actual.shape[1])])\n",
    "\n",
    "# for i in range(len(res_df)):\n",
    "# #   print(i)\n",
    "plt.scatter(x=predicted_df['f0'],y=predicted_df['f1'],c='r',s=0.1,alpha=0.5,label='Predicted')\n",
    "plt.scatter(x=actual_df['f0'],y=actual_df['f1'],c='b',s=0.1,alpha=0.5,label='Actual')\n",
    "plt.title('Stacking')\n",
    "plt.xlabel('Global X(m)')\n",
    "plt.ylabel('Global Y(m)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.05824850443744229\n",
      "MAE: 0.08155734364760017\n",
      "相关系数为： 0.9597269629903602\n",
      "R^2: 0.9999973385174523\n",
      "Correlation coefficient: 0.9597269629903602\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# # 定义真实定位数据和预测数据\n",
    "# true_pos = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "# pred_pos = np.array([[0.5, 1.5], [3.2, 3.9], [4.9, 6.2]])\n",
    "\n",
    "# 计算均方误差\n",
    "mse = mean_squared_error(actual_df, predicted_df)\n",
    "print(\"MSE:\", mse)\n",
    "\n",
    "# 计算平均绝对误差\n",
    "mae = mean_absolute_error(actual_df, predicted_df)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "# 计算相关系数矩阵\n",
    "corr_matrix = np.corrcoef(actual_df.T, predicted_df.T)\n",
    "corr_coefficient = corr_matrix[0, 1] # 取出相关系数值\n",
    "print(\"相关系数为：\", corr_coefficient) # 打印相关系数\n",
    "# 计算决定系数\n",
    "r2 = r2_score(actual_df, predicted_df)\n",
    "print(\"R^2:\", r2)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 将实际值和预测值分别存储在矩阵中\n",
    "actual_matrix = np.array(actual_df)\n",
    "predicted_matrix = np.array(predicted_df)\n",
    "correlation_matrix = np.corrcoef(actual_matrix, predicted_matrix, rowvar=False) # 计算相关系数（Pearson相关系数）\n",
    "correlation_coefficient = correlation_matrix[0, 1] # 提取相关系数矩阵中的相关系数\n",
    "print(\"Correlation coefficient:\", correlation_coefficient) # 打印相关系数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[679955.8413     419832.253      679955.84394513 419832.20976238]\n",
      " [679955.8419     419832.2448     679955.84394513 419832.20976238]\n",
      " [679878.8793     419458.8907     679878.85980691 419458.96066182]\n",
      " ...\n",
      " [679955.8474     419832.1637     679955.84496274 419832.19322956]\n",
      " [679955.844      419832.2116     679955.84435727 419832.20319948]\n",
      " [679955.8437     419832.2183     679955.84394513 419832.20976238]]\n"
     ]
    }
   ],
   "source": [
    " #合并两个矩阵\n",
    "merged_matrix = np.concatenate((y_test_actual, res_df), axis=1)#真实,预测\n",
    "\n",
    "print(merged_matrix) # 输出合并后矩阵的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04331846 0.03509726 0.07262669 0.00013264 0.00283367 0.35047308\n",
      " 0.68365925 0.00283367 0.00283367 0.00821109 0.04487168 0.07832864\n",
      " 0.00283367 0.03082606 0.13332039 0.02651873 0.28491801 0.00445074\n",
      " 0.04798802 0.01308535 0.00283367 0.18812035 0.12558131 0.84489688\n",
      " 0.01925121 0.07386033 0.04797452 0.49733769 0.00283367 0.00283367\n",
      " 0.00448636 0.01732215 0.12392264 0.00283367 0.00809754 0.01053542\n",
      " 0.00283367 1.11110441 0.40764407 0.00283367 0.0089235  0.17484597\n",
      " 0.02646946 0.91179295 0.12703731 0.16803351 0.17882052 1.72687736\n",
      " 0.00283367 0.00375724 0.10851737 0.00283367 0.18023142 0.52722629\n",
      " 0.77662807 0.04216184 1.05394532 0.70755511 0.00041632 0.0540412\n",
      " 1.5340531  0.00900531 1.90562105 0.02715415 0.04146408 0.01736861\n",
      " 0.0289902  0.3139693  0.00092563 0.02448537 0.00283367 0.02945679\n",
      " 0.00283367 0.07546932 0.00283367 0.0349565  0.00812438 0.00313821\n",
      " 0.00283367 0.02640469 0.60489082 0.01765786 0.01470838 0.00241515\n",
      " 0.00283367 0.08916224 0.18728253 0.341252   1.87636293 0.85763812\n",
      " 0.01182049 0.33319754 0.04236297 1.3177791  0.01650132 1.45058849\n",
      " 0.02571268 0.02338434 0.00283367 0.44688683 0.09927928 0.02128269\n",
      " 0.08550486 0.00544315 0.0382788  0.0090781  0.08181847 0.63078844\n",
      " 0.01231549 0.02840067 0.06259422 0.06252652 0.0225712  0.08736703\n",
      " 0.01253564 0.05353402 0.05614965 0.00283367 0.90068876 0.00073671\n",
      " 0.84884977 0.07464582 0.89807874 0.01562348 0.07323433 0.02808065\n",
      " 0.01968907 0.02171569 0.07820054 0.00283367 0.03612523 0.29936772\n",
      " 1.47103395 0.00380294 0.2978216  0.00671747 0.00283367 0.01448437\n",
      " 0.05313681 0.09987778 0.01162547 0.336349   0.00132356 0.3105547\n",
      " 0.01537081 0.08365499 0.11662083 0.28554332 0.71908904 0.01764622\n",
      " 0.30398835 0.00283367 0.00283367 0.84032816 0.00283367 0.32484611\n",
      " 0.01535716 0.11326957 0.50113995 0.00884563 0.08856279 0.08410661\n",
      " 0.02687563 0.06447102 0.15060435 0.10660348 0.11373455 0.00548284\n",
      " 0.2769933  0.02160781 0.00283367 0.08425338 0.01070552 0.00283367\n",
      " 0.0018163  0.00620122 0.00283367 0.00283367 0.1333438  0.02106877\n",
      " 0.06938017 0.06859843 0.44161642 0.00283367 0.10965778 0.00180932\n",
      " 0.73729828 0.06508323 0.04564298 0.00055161 0.00821995 0.45624435\n",
      " 0.00214117 0.00283367 0.01094031 0.00650176 0.00328601 0.03943909\n",
      " 0.00283367 0.01405515 0.04177375 0.07359077 0.02657694 0.00283367\n",
      " 0.02312584 2.13071242 0.82149638 0.01838577 0.02453624 0.0928282\n",
      " 0.15465096 0.00283367 0.03342844 0.03533821 0.00279531 0.00283367\n",
      " 0.43069513 0.00283367 0.00910804 2.02028853 0.52740175 0.02290337\n",
      " 0.00914091 0.04395643 0.00283367 0.496461   0.00339601 0.06954867\n",
      " 0.63101135 0.00083608 0.06813885 0.01590034 1.33807568 0.00283367\n",
      " 0.00283367 0.06692591 0.39540351 0.61677863 0.10348999 0.06816239\n",
      " 0.1165544  0.0193937  0.04643112 0.00283367 0.02711392 0.0084815\n",
      " 0.00062523 0.00207819 0.08305662 0.08674693 0.00110656 0.62538614\n",
      " 0.01328587 0.41194353 0.06093628 0.01909756 0.10909805 0.11847812\n",
      " 0.00298871 0.07280862 0.00283367 0.3427612  0.00175985 0.00462177\n",
      " 0.00283367 0.02408103 0.00389235 0.03995377 0.81177478 0.00235059\n",
      " 0.00114031 0.00681856 0.02325774 0.14430028 0.01221648 0.00283367\n",
      " 0.01099937 0.28530065 0.39495606 0.01036875 0.00014474 0.00060268\n",
      " 0.00283367 0.00510928 0.04822763 0.00283367 0.03964607 0.00283367\n",
      " 0.12877912 0.01461378 0.0050007  0.00794687 0.02614271 0.12980397\n",
      " 0.02202854 0.02310514 0.22614317 0.05684827 0.00283367 0.01409079\n",
      " 0.12709888 0.00283367 0.00283367 0.00129093 0.28633096 0.12152979\n",
      " 0.01680062 0.14425538 0.00283367 0.10295231 0.04455472 0.01040339\n",
      " 0.01033697 0.07934457 0.00283367 0.00283367 0.02472212 0.00891388\n",
      " 0.04762877 0.63141235 0.00292031 0.37289494 0.00785593 0.08119452\n",
      " 0.01131637 0.00313275 0.01888954 0.82551379 0.03655431 0.34708153\n",
      " 0.00410207 0.05081904 0.01051421 0.02958913 0.00283367 0.02379604\n",
      " 0.52694481 0.07692197 0.00283367 0.01764254 0.02119528 0.05247747\n",
      " 0.01214389 0.61345879 0.00970162 0.10651163 0.11542386 0.04152196\n",
      " 0.00450741 0.11207206 0.46145119 0.00556574 0.10456066 0.01012542\n",
      " 0.00446554 0.01120375 0.06016063 0.32510985 0.00283367 0.13322496\n",
      " 0.2014868  0.01368515 0.01212452 0.00283367 0.03751176 0.35313221\n",
      " 0.05078557 0.03065337 0.00956276 0.10408848 0.09010406 0.18804017\n",
      " 0.00583891 0.0793459  1.29936056 0.02487088 0.08431148 0.0129304\n",
      " 0.01417045 0.00369729 0.21298898 0.00288649 0.01479026 0.10536676\n",
      " 0.00374491 0.0280681  0.03501503 0.00283367 0.00283367 0.01271297\n",
      " 0.23957685 0.05559128 0.00283367 0.00139425 0.07772071 0.00283367\n",
      " 0.02717607 0.02422093 0.00283367 0.04886802 0.18013661 0.00623717\n",
      " 0.06540058 0.0081084  0.0262968  0.00920053 0.00283367 0.00606816\n",
      " 0.01007078 0.74314758 0.02556686 0.03773662 0.00350324 0.00283367\n",
      " 0.02235348 0.00973647 0.09222944 0.15345464 0.07593837 0.54064042\n",
      " 0.00283367 0.0188827  0.69393354 0.01319055 0.03693114 0.07946537\n",
      " 0.0469706  0.00283367 0.00785239 0.05008296 0.01947894 0.17754184\n",
      " 0.15850557 0.01628413 0.00283367 0.06661176 0.0717754  0.07234578\n",
      " 0.0149552  0.00340921 0.0283343  0.29579559 1.05857169 0.00283367\n",
      " 0.01273871 0.14988374 0.0233419  0.53369058 0.06413444 0.00597687\n",
      " 0.00283367 0.00050692 0.10786145 0.01645934 0.07536644 0.00674208\n",
      " 0.00283367 0.00472303 0.00283367 0.01990049 0.00283367 0.00283367\n",
      " 0.0277121  0.00283367 0.01690554 0.06135822 0.53344476 0.01733913\n",
      " 0.00283367 0.05000259 0.67846603 0.01657387 0.46220572 0.70770954\n",
      " 0.00283367 0.00283367 0.03461276 0.04359132 0.10839893 0.09896528\n",
      " 0.01493963 0.00283367 0.53984848 0.0186158  0.00907807 0.10846021\n",
      " 0.84320209 0.0100379  0.08732029 0.00448707 0.01880485 0.00283367\n",
      " 0.01549257 0.00687421 0.07538711 0.00283367 0.04678851 0.06075941\n",
      " 0.02041473 0.0120711  0.00283367 1.06302213 0.01579162 0.0158446\n",
      " 0.20907473 0.0241078  0.01355676 0.01981492 0.00283367 0.02647116\n",
      " 0.00707052 0.32579664 0.07153352 0.03334218 2.48166815 0.01419562\n",
      " 0.17976023 0.00283367 0.01041501 0.02482095 0.0848518  0.04030591\n",
      " 0.80940996 0.1168414  0.03952609 0.04013535 0.30771712 0.35815547\n",
      " 1.12084985 0.00141812 0.00310364 0.36945453 0.00283367 0.19622686\n",
      " 0.00283367 0.00283367 0.01330449 0.04702993 0.12093128 0.41615758\n",
      " 0.00985464 0.07964254 0.02246948 1.36514065 0.12118197 0.02175461\n",
      " 0.00283367 0.00283367 0.00363804 0.01513653 0.00283367 0.1227885\n",
      " 0.10043825 0.0485112  0.00470803 0.00283367 0.00283367 0.00283367\n",
      " 0.47232238 0.014714   0.17235714 0.00283367 0.11323263 0.04882653\n",
      " 0.04748276 0.00283367 0.01005743 0.1016946  0.1145863  0.74595663\n",
      " 0.0663185  0.07482737 0.00283367 0.02544705 0.03181036 0.02232433\n",
      " 0.0070256  0.00781278 0.40043684 1.3762303  0.41421743 0.00283367\n",
      " 0.13130948 0.00283367 0.12218668 0.01631017 0.00283367 0.39602648\n",
      " 0.12464324 0.05744367 0.00283367 0.00541402 0.00433271 0.0396208\n",
      " 0.02760302 0.56594752 0.52366169 1.59138463 0.65680383 0.10899745\n",
      " 0.01397576 0.04343744 0.04511077 0.00741151 0.40856142 0.00283367\n",
      " 0.00227262 0.00347048 0.07802336 0.0953232  0.06379112 0.01829426\n",
      " 0.00283367 0.00319079 0.02126288 0.00215314 0.02858525 0.00479427\n",
      " 0.06560813 0.00971449 0.02194043 0.00594892 0.00283367 0.00730457\n",
      " 0.02210186 0.09201453 0.01439676 0.08179337 0.00283367 0.1203935\n",
      " 0.1004763  0.00283367 0.23138477 0.52785567 0.04790174 0.12340672\n",
      " 0.09204886 0.02647803 0.00799364 0.39637662 0.12891454 2.06490016\n",
      " 0.00283367 0.05134029 0.08796541 0.00385445 0.03761081 0.14360313\n",
      " 0.01479145 0.00283367 0.03970694 0.02487903 0.00283367 0.02503353\n",
      " 0.04300687 0.00283367 0.08981521 0.00283367 0.00283367 0.09041403\n",
      " 0.02564505 0.00283367 0.05360257 0.05433462 0.00283367 0.00840151\n",
      " 0.00283367 0.03377938 0.01551815 0.0184183  0.01227416 0.01585377\n",
      " 1.28719387 0.01936256 0.00145617 0.00283367 0.41374582 0.02314718\n",
      " 0.02691634 0.06139744 0.00487142 0.04172569 0.03473914 0.04950781\n",
      " 0.12640033 0.00873437 0.37288374 0.06810157 0.06866286 0.00182108\n",
      " 0.00283367 0.00283367 0.00283367 0.0040294  0.04057157 0.54673696\n",
      " 0.03324245 0.0216679  0.01265288 0.0896737  0.04022738 0.23867839\n",
      " 0.00283367 0.00975997 0.00784819 0.01698083 0.16529241 0.00988772\n",
      " 0.00283367 0.12272686 0.00283367 0.07391596 0.00672488 0.00283367\n",
      " 0.00283367 0.00194102 0.38255837 0.25487089 0.13011199 0.03212507\n",
      " 0.02962997 0.00840812 0.00854114]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# 计算每行数据中两个点之间的欧式距离\n",
    "distances = np.linalg.norm(merged_matrix[:, :2] - merged_matrix[:, 2:], axis=1)\n",
    "print(distances)\n",
    "np.savetxt('distances.txt', distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean distance: 0.1434698932714909\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "distances = np.loadtxt('distances.txt')\n",
    "mean_distance = np.mean(distances)\n",
    "\n",
    "print('Mean distance:', mean_distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
